1) Input sensors

Cameras, RADAR, LIDAR, and GNSS are the most common sensors used.

Camera feeds can be hard to work with. 
Image stitching increases the real-time processing load. 
The quality of the images we obtain may not be consistent: if the lighting is poor, then shadow effects and lots of other details enter the picture that our system may not be able to handle with ease. 
Good quality cameras are needed. They must function even under direct sunlight, and must be able to take very clear shots.
Camera feeds are still incredibly useful, because they often capture crucial information we cannot obtain through other input methods. ( a visual feed is almost everything a real human uses to drive a vehicle) 

RADAR and LIDAR
Both these sensors work by receiving reflected transmissions. 
RADAR uses a high wavelength, and cannot offer much detail about objects, whereas the high precision of LIDAR allows us even to make 3D models of the environment.
RADAR can see through fog, etc where LIDAR doesn't perform well. LIDAR scanners are more sensitive and end up being a bit more expensive.

GNSS may not be reliable in some locations. The data supplied by it alone cannot form a reliable system to drive an autonomous vehicle. 
Sensor fusion is desirable to ensure that the vehicle can handle any environment. Only then can we consider an autonomous vehicle safe for general use.
Sensor fusion is NOT a redundancy system. We use multiple sensors using a common time reference, and are caliberated to capture data in a common region in the physical world. 
A multi branch nueral network is designed to use all the sensor data to make a single final decision. 
Each sensors input is given a feature map separately, and then these are merged together before the neural network is trained. 
This is one major way to implement sensor fusion. 
Extremely similar inputs can be merged together in the input stage itself, and be given one single feature map. Eg: IR and camera feeds can be effectively merged.
There are some other methods too, but the basic idea is to use the proficiencies of each sensor to form a collectively complete detection.
------------------------
2)3D LIDAR Point Cloud-based Intersection Recognition for Autonomous Driving

• The input that we are taking is from a LIDAR scanner. It gives us the closest reflecting surface at every angle.
This will give us a 3D point cloud of data, where we can imagine we keep a dot at every point that has an object.
• We look at the Bird's eye 2D view of this data, and draw a grid to aid us, with the smallest squares being of some dimension r x r.
• Each square is now a thin cuboid of data, with dots in places where stuff exists. We take the variance of the heights of these data points.
If the variance crosses a certain threshold, we flag the grid cell as 1 (meaning "stuff is here.") If it’s low variance data, we flag it as 0.

---Quick Remark---
1) Low variance objects may exist. We don't care.

If there is some planar object, say, a large metal sheet, or even a rod, sticking out of a truck, that is NOT going to get detected.
Whether or not the car drives into such a rod, killing the passenger, is none of our concern.
The real objective here is to detect intersections. 
The curb of the road is what we need. We are flagging the potentially relevant stuff, and then removing the unnecessary data, namely, pedestrians, and vehicles.

2) Important notice: We rely on the fact that there IS a curb bounding the roads. 
If the roads do not have a curb, or are in a flat area with little surrounding terrain, since the algorithm doesn’t look AT the road, but instead what BOUNDS the road, it’s going to face total failure. 
This is kind of a big deal.
Suggestion: Also use some camera feed data to build a more descriptive picture of the environment, and establish a robust system.
---End of Remark---

• We have a 2D binary map of the stuff around the vehicle. Now, we try to filter out the things that are NOT the curb. 
To do this, we first search for dense blobs of data, by traversing the map, and picking out 4 connected regions that are all totally flagged as 1. 
Now, we use these to draw rectangles that bound the dense blobs. 
Even parts of the curb may get selected here, so we use the dimensions of this rectangle, to decide which ones are the objects that need to be removed.

• Now, we have just the curb left, and this will be used for the actual intersection Detection.
• We frame this as a classification problem and train a Support Vector Machine classifier to do the job.
• To implement this, we need to convert this 2D map into more concise and relevant tabulated data. 
To do this, we take the distance of the nearest object from the "launch point", at every angle, 0 to 359 degrees. 
This length, when normalised, gives us our data. 
•The launch point itself, of course, needs to be fixed to accomplish this. 
Since the vehicle is moving as it emits and receives the laser, we consider the launch point to be somewhere ahead of the vehicle, and how far away the point is, depends on the speed of the car. In the paper they use D = 5 + v*t . t=1s

Suggestion: It may not be reasonable to expect the velocity to be a constant over a span of one second. A shorter time interval can be used depending on the variance of the velocity, and the constraints of the scanner.

•The classifier will first decide if it’s a straight road or an intersection, and then if it’s an intersection, it determines if it’s a T or a + shaped one. Mission Accomplished!
------------------------
